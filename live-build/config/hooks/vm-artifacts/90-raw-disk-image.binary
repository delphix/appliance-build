#!/bin/bash -ex
#
# Copyright 2018 Delphix
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

#
# This script is intended to be used as part of Delphix's build process.
# It's role is to convert the "binary" directory generated by live-build,
# into a bootable disk image, running ZFS as the root filesystem (whose
# contents are that of the "binary" directory).
#

die() {
	echo "$*" 1>&2
	exit 1
}

#
# The root filesystem container needs to have the appliance version
# embedded in it as a dataset property, thus if this value was not
# passed into the build, we need to error out (preferably before we do
# anything else, to avoid having to do cleanup later). If the user
# executing the build did not pass this in, the build system should
# provide a default value (which is why we don't attempt to assign a
# default value here).
#
[[ -n "$DELPHIX_APPLIANCE_VERSION" ]] ||
	die "DELPHIX_APPLIANCE_VERSION variable is missing"

#
# We want to use different sized rpool depending on if we're building a
# disk image meant for internal use, or external (i.e. customer) use.
#
# The only exception to this is our "dcenter" variant. While that
# variant is only used internally, we use it in a way that more
# resembles our external variants, so we want the rpool size for
# the dcenter images to match our external images.
#
case "$APPLIANCE_VARIANT" in
external-* | internal-dcenter)
	RAW_DISK_SIZE_GB=127
	;;
internal-*)
	RAW_DISK_SIZE_GB=70
	;;
*)
	die "Invalid variant specified: '$APPLIANCE_VARIANT'"
	;;
esac

rm -f "$ARTIFACT_NAME.img"
truncate -s "${RAW_DISK_SIZE_GB}G" "$ARTIFACT_NAME.img"
sgdisk --zap-all "$ARTIFACT_NAME.img"

#
# Here we're creating the boot partition. When installing grub, this
# partition will be used and automatically detected by "grub-install"
# based on the partitions typecode. This partition is required since we're
# partitioning using GPT; if we used MBR, an explicit boot partition
# wouldn't be required. Also we leave the first 1MB unallocated since
# some clouds (i.e. Azure)  may require space for their own internal
# purposes.
#
sgdisk "$ARTIFACT_NAME.img" \
	--set-alignment=1 --new=2:1m:+1m --typecode=2:EF02

#
# Now we create the partition that we'll use for the zpool that will be
# used for the root pool. We use a generic typecode for this partition
# that simply maps to "Linux filesystem". The typecode here is simply
# cosmetic, as there's nothing that relies on it being set.
#
sgdisk "$ARTIFACT_NAME.img" --new=1:: --typecode=1:8300

#
# This is done simply for debugging and/or diagnostic purposes. When
# running this script via automation, it can be helpful to capture the
# state of the disk image's partition table in the logs.
#
sgdisk "$ARTIFACT_NAME.img" --print

#
# We expect kpartx's output to resemble the following:
#
#     add map loop0p1 (253:0): 0 33552351 linear 7:0 2048
#     add map loop0p2 (253:1): 0 2014 linear 7:0 34
#
# We then manipulate this output, such that we can return only the
# loopback device name, which would be "loop0" in this example. From that,
# consumers can easily build the names of the individual partitions as
# needed.
#
LOOPNAME=$(kpartx -asv "$ARTIFACT_NAME.img" |
	head -n1 |
	awk '{ print $3 }' |
	sed 's/^\(loop[0-9]\+\)p[0-9]\+$/\1/')

#
# We use a consistent naming scheme for the root filesystems that are
# generated here, on initial build, but also on upgrade and/or
# migration. Thus, if the name is changed here, we also need to update
# the name used during upgrade so the names remain consistent.
#
DIRECTORY=$(mktemp -d -p "/mnt" -t delphix.XXXXXXX)
FSNAME=$(basename "$DIRECTORY")

zpool create -d \
	-o version=28 \
	-O canmount=off \
	-O mountpoint=none \
	-O compression=on \
	-R "$DIRECTORY" \
	rpool "/dev/mapper/${LOOPNAME}p1"

zfs create \
	-o canmount=off \
	-o mountpoint=none \
	"rpool/ROOT"

zfs create \
	-o canmount=off \
	-o mountpoint=none \
	-o "com.delphix:initial-version=$DELPHIX_APPLIANCE_VERSION" \
	-o "com.delphix:current-version=$DELPHIX_APPLIANCE_VERSION" \
	"rpool/ROOT/$FSNAME"

if [[ -n "$DELPHIX_HOTFIX_VERSION" ]]; then
	zfs set \
		"com.delphix:hotfix-version=$DELPHIX_HOTFIX_VERSION" \
		"rpool/ROOT/$FSNAME"
fi

if [[ -n "$DELPHIX_MINIMUM_VERSION" ]]; then
	zfs set \
		"com.delphix:minimum-version=$DELPHIX_MINIMUM_VERSION" \
		"rpool/ROOT/$FSNAME"
fi

zfs create \
	-o canmount=noauto \
	-o mountpoint=/ \
	"rpool/ROOT/$FSNAME/root"

zfs mount "rpool/ROOT/$FSNAME/root"

#
# We are later going to recursively bind mount /proc/, /sys/, and /dev/
# beneath the root dataset. Before doing that, we need to change the root
# dataset's mount so that it has type 'slave'. If were to leave it 'shared',
# the following would happen:
#
#  - The mount of $DIRECTORY/sys/ would be propagated to other existing
#    namespaces.
#  - We would recursively change the propagation type of $DIRECTORY/sys/ to
#    be 'slave', for reasons explained below.
#  - After finishing our work, we would 'umount -R $DIRECTORY/sys/'. For the
#    children operated on recursively, this event would _not_ be propagated
#    because they are not longer shared.
#  - We would unmount $DIRECTORY. This would succeed, but propagation of it
#    would silently fail, because it is busy in other namespaces, because
#    $DIRECTORY/sys/ is still mounted on it.
#  - We would export the pool, which would fail because the pool is busy
#    because root filesystem is still mounted in other namespaces.
#
# To prevent this, we can change the propagation type of $DIRECTORY so that
# the bind mount of /sys and the others is never propagated to other
# namespaces.
#
# Also, since we are going to change the propagation type, we need to change
# it before before mounting _any_ children. Otherwise we would end up in the
# same situation, but with a different child: the mount would be proagated,
# the unmount would not, and we would end up with EBUSY errors when exporting
# the pool because some filesystems are mounted in other namespaces.
#
mount --make-slave "$DIRECTORY"

zfs create \
	-o mountpoint=legacy \
	"rpool/ROOT/$FSNAME/home"

zfs create \
	-o mountpoint=legacy \
	"rpool/ROOT/$FSNAME/data"

zfs create \
	-o mountpoint=legacy \
	-o recordsize=1MB \
	"rpool/ROOT/$FSNAME/log"

#
# Initialize the grub dataset. This dataset will be used to contain all
# of the grub-specific files; this includes the "grub.cfg" file, along
# with the files created when running "grub-install".
#
# We maintain a seperate dataset for grub, so that we can maintain it
# differently than how we maintain and version the root filesystems.
# This allows us to have a single configuration for the bootloader,
# spanning all of the root filesystems that may be present on the
# system; rather than trying to have a configuration on each root
# filesystem, and trying to keep them all consistent with each other.
#
# Additionally, we use the "legacy" mountpoint so we can carefully
# control when this dataset is mounted, to help ensure the dataset isn't
# modified unexpectedly. Thus, we mount it on-demand when we need to
# make changes to it, and then quickly unmount it.
#
zfs create \
	-o mountpoint=legacy \
	rpool/grub

#
# Initialize the crashdump dataset. This is used to store core files
# from processes that have crashed. Since we don't have control on how
# many of these core files accumulate, we set a reasonable quota (50% of
# the rpool's size) to keep these from running the rpool out of space.
#
zfs create \
	-o mountpoint=legacy \
	-o quota="$(echo "$(zpool list -Hpo size rpool) / 2" | bc)b" \
	rpool/crashdump

#
# Since these datasets use "legacy" for their mountpoints, we need to
# explicitly mount them now, before we rsync over the "binary" directory
# contents. During normal boot up, we'll rely on "/etc/fstab" to handle
# these mounts.
#
mkdir -p "$DIRECTORY/export/home"
mount -t zfs "rpool/ROOT/$FSNAME/home" "$DIRECTORY/export/home"

mkdir -p "$DIRECTORY/var/delphix"
mount -t zfs "rpool/ROOT/$FSNAME/data" "$DIRECTORY/var/delphix"

mkdir -p "$DIRECTORY/var/log"
mount -t zfs "rpool/ROOT/$FSNAME/log" "$DIRECTORY/var/log"

mkdir -p "/var/crash"
mount -t zfs "rpool/crashdump" "/var/crash"

#
# Populate the root filesystem with the contents of the "binary" directory
# that (we assume) was previously generated by live-build.
#
rsync --info=stats3 -Wa binary/* "$DIRECTORY/"

#
# We rely on the "/etc/fstab" file to mount the non-root ZFS
# filesystems, so that when a specific rootfs dataset is booted, it'll
# automatically mount the correct non-rootfs datasets associated with
# that rootfs dataset. We make sure that local mounts run before the
# zfs-import service since that service can hold the 'spa_namespace_lock'
# preventing legacy zfs mounts from completing. In addition, we need to
# mount /var/crash before the 'kdump-tools' service since that service
# needs to write into that directory. The /var/crash filesystem must
# exist in /etc/fstab since that will ensure that it gets mounted
# automatically whenever we boot into the crash kernel.
#
cat <<-EOF >"$DIRECTORY/etc/fstab"
	rpool/ROOT/$FSNAME/home /export/home zfs defaults,x-systemd.before=zfs-import-cache.service 0 0
	rpool/ROOT/$FSNAME/data /var/delphix zfs defaults,x-systemd.before=zfs-import-cache.service 0 0
	rpool/ROOT/$FSNAME/log  /var/log     zfs defaults,x-systemd.before=zfs-import-cache.service 0 0
	rpool/crashdump  /var/crash     zfs defaults,x-systemd.before=zfs-import-cache.service,x-systemd.before=kdump-tools.service 0 0
EOF

#
# Now we need to install the bootloader. In order to do that, we'll chroot
# into the newly populated root filesystem, so that we use the grub-install
# and update-grub binaries installed in that filesystem.  Additionally, we
# need to have the /dev, /proc, and /sys mountpoints present in that chroot
# environment, which is why we bind mount here.
#
for dir in /dev /proc /sys; do
	mount --rbind "$dir" "${DIRECTORY}${dir}"
	#
	# Bind mounts are placed in the same peer group as the mount being
	# copied. This means that when we later need to 'umount -R' this
	# directory, the unmount events for any children of this mount will
	# be propagated to the original mount point. So, for instance, when
	# we unmount $DIRECTORY/sys/fs/cgroup, that will also attempt to
	# unmount /sys/fs/cgroup. To prevent this from happening, we need to
	# change the mount propagation type to prevent the unmount from being
	# propagated.
	#
	mount --make-rslave "${DIRECTORY}${dir}"
done

#
# We need to use the dedicated grub dataset when running "grub-install"
# and "grub-mkconfig", so we need to mount this dataset first.
#
chroot "$DIRECTORY" mount -t zfs rpool/grub /mnt
chroot "$DIRECTORY" grub-install --root-directory=/mnt "/dev/$LOOPNAME"
chroot "$DIRECTORY" grub-mkconfig -o /mnt/boot/grub/grub.cfg
chroot "$DIRECTORY" umount /mnt

for dir in /dev /proc /sys; do
	for attempt in {1..5}; do
		umount -R "${DIRECTORY}${dir}" && break
		[[ "$attempt" == 5 ]] && die "Too many failed attempts, aborting."
		echo "Attempt $attempt failed, trying again after a small nap."
		sleep 10
	done
done

umount "$DIRECTORY/var/log"
umount "$DIRECTORY/var/delphix"
umount "$DIRECTORY/export/home"
umount "/var/crash"
zfs umount "rpool/ROOT/$FSNAME/root"
zpool export rpool
kpartx -d "$ARTIFACT_NAME.img"
