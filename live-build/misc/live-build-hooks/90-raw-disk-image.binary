#!/bin/bash -ex
#
# Copyright 2018 Delphix
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

#
# This script is intended to be used as part of Delphix's build process.
# It's role is to convert the "binary" directory generated by live-build,
# into a bootable disk image, running ZFS as the root filesystem (whose
# contents are that of the "binary" directory).
#

die() {
	echo "$*" 1>&2
	exit 1
}

#
# We want to use different sized rpool depending on if we're building a
# disk image meant for internal use, or external (i.e. customer) use.
#
case "$APPLIANCE_VARIANT" in
internal-*)
	RAW_DISK_SIZE_GB=70
	;;
external-*)
	RAW_DISK_SIZE_GB=127
	;;
*)
	die "Invalid variant specified: '$APPLIANCE_VARIANT'"
	;;
esac

rm -f "$APPLIANCE_VARIANT.img"
truncate -s "${RAW_DISK_SIZE_GB}G" "$APPLIANCE_VARIANT.img"
sgdisk --zap-all "$APPLIANCE_VARIANT.img"

#
# Here we're creating the boot partition. When installing grub, this
# partition will be used and automatically detected by "grub-install"
# based on the partitions typecode. This partition is required since we're
# partitioning using GPT; if we used MBR, an explicit boot partition
# wouldn't be required.
#
sgdisk "$APPLIANCE_VARIANT.img" \
	--set-alignment=1 --new=2:34:2047 --typecode=2:EF02

#
# Now we create the partition that we'll use for the zpool that will be
# used for the root pool. We use a generic typecode for this partition
# that simply maps to "Linux filesystem". The typecode here is simply
# cosmetic, as there's nothing that relies on it being set.
#
sgdisk "$APPLIANCE_VARIANT.img" --new=1:: --typecode=1:8300

#
# This is done simply for debugging and/or diagnostic purposes. When
# running this script via automation, it can be helpful to capture the
# state of the disk image's partition table in the logs.
#
sgdisk "$APPLIANCE_VARIANT.img" --print

#
# We expect kpartx's output to resemble the following:
#
#     add map loop0p1 (253:0): 0 33552351 linear 7:0 2048
#     add map loop0p2 (253:1): 0 2014 linear 7:0 34
#
# We then manipulate this output, such that we can return only the
# loopback device name, which would be "loop0" in this example. From that,
# consumers can easily build the names of the individual partitions as
# needed.
#
LOOPNAME=$(kpartx -asv "$APPLIANCE_VARIANT.img" |
	head -n1 |
	awk '{ print $3 }' |
	sed 's/^\(loop[0-9]\+\)p[0-9]\+$/\1/')

#
# We use a consistent naming scheme for the root filesystems that are
# generated here, on initial build, but also on upgrade and/or
# migration. Thus, if the name is changed here, we also need to update
# the name used during upgrade so the names remain consistent.
#
# shellcheck disable=SC2016
VERSION=$(chroot binary dpkg-query -Wf '${Version}' delphix-entire)
DIRECTORY=$(mktemp -d -p "/mnt" -t "$VERSION.XXXXXXX")
FSNAME=$(basename "$DIRECTORY")

zpool create -d \
	-o version=28 \
	-O canmount=off \
	-O mountpoint=none \
	-R "$DIRECTORY" \
	rpool "/dev/mapper/${LOOPNAME}p1"

zfs create \
	-o canmount=off \
	-o mountpoint=none \
	"rpool/ROOT"

zfs create \
	-o canmount=off \
	-o mountpoint=none \
	"rpool/ROOT/$FSNAME"

zfs create \
	-o canmount=noauto \
	-o compression=on \
	-o mountpoint=/ \
	"rpool/ROOT/$FSNAME/root"

zfs mount "rpool/ROOT/$FSNAME/root"

zfs create \
	-o canmount=on \
	-o compression=on \
	-o mountpoint=/export/home \
	"rpool/ROOT/$FSNAME/home"

zfs create \
	-o canmount=on \
	-o compression=on \
	-o mountpoint=/var/delphix \
	"rpool/ROOT/$FSNAME/data"

#
# Initialize the crashdump dataset. This is used to store core files
# from processes that have crashed. Since we don't have control on how
# many of these core files accumulate, we set a reasonable quota (25% of
# the rpool's size) to keep these from running the rpool out of space.
#
zfs create \
	-o canmount=on \
	-o compression=on \
	-o mountpoint=/var/crash \
	-o quota="$(echo "$(zpool list -Hpo size rpool) / 4" | bc)b" \
	rpool/crashdump

#
# Populate the root filesystem with the contents of the "binary" directory
# that (we assume) was previously generated by live-build.
#
rsync --info=stats3 -Wa binary/* "$DIRECTORY/"

#
# Now we need to install the bootloader. In order to do that, we'll chroot
# into the newly populated root filesystem, so that we use the grub-install
# and update-grub binaries installed in that filesystem.  Additionally, we
# need to have the /dev, /proc, and /sys mountpoints present in that chroot
# environment, which is why we bind mount here.
#
mount --make-slave "$DIRECTORY"
for dir in /dev /proc /sys; do
	mount --rbind "$dir" "${DIRECTORY}${dir}"
	mount --make-rslave "${DIRECTORY}${dir}"
done

chroot "$DIRECTORY" grub-install "/dev/$LOOPNAME"
chroot "$DIRECTORY" update-grub

for dir in /dev /proc /sys; do
	umount -R "${DIRECTORY}${dir}"
done

zfs umount rpool/crashdump
zfs umount "rpool/ROOT/$FSNAME/data"
zfs umount "rpool/ROOT/$FSNAME/home"
zfs umount "rpool/ROOT/$FSNAME/root"
zpool export rpool
kpartx -d "$APPLIANCE_VARIANT.img"
