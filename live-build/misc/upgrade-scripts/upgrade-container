#!/bin/bash
#
# Copyright 2018 Delphix
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

. "${BASH_SOURCE%/*}/common.sh"

IMAGE_VERSION=$(get_image_version)
[[ -n "$IMAGE_VERSION" ]] || die "failed to determine image version"

CONTAINER=

function create_cleanup() {
	#
	# Upon successful creation of the container, don't perform any
	# cleanup logic. Rather, the consumer of the script should
	# eventually "destroy" the container.
	#
	# shellcheck disable=SC2181
	[[ "$?" -eq 0 ]] && return

	#
	# If the CONTAINER variable is empty, this means the create
	# function failed before the container directory could be
	# created. Thus, there's nothing to clean up, so we can
	# immediately exit.
	#
	[[ -z "$CONTAINER" ]] && return

	destroy "$CONTAINER"
}

function create_upgrade_container_common() {
	mkdir -p "/etc/systemd/nspawn" ||
		die "failed to create directory: '/etc/systemd/nspawn'"

	#
	# We need to change some of the default configuration options
	# for the container, in order for it to behave as we expected:
	#
	# * PrivateUsers: We want the UIDs in the container to match the
	#   UIDs in the host. This way the container can execute commands
	#   (e.g. zfs) as the normal root user.
	#
	# * PrivateUsersChown: Since we set PrivateUsers to "no", we
	#   need to also ensure systemd-nspawn doesn't attempt to chown
	#   the containers root filesystem. Normally, with PrivateUsers
	#   set to "yes", the container's root filesystem would be
	#   chowned to match the UID mapping used by the container.
	#
	# * Private: We set networking to "private" so that the
	#   container can use ports without conflicting with ports that
	#   are already in used by the host.
	#
	# * Bind=/dev/zfs: We set this so that zfs/zpool/libzpool/etc.
	#   is usable from within the container.
	#
	cat >"/etc/systemd/nspawn/$CONTAINER.nspawn" <<-EOF ||
		[Exec]
		PrivateUsers=no
		[Network]
		Private=yes
		[Files]
		PrivateUsersChown=no
		Bind=/dev/zfs
	EOF
		die "failed to create container configuration file"

	#
	# Since UPDATE_DIR is stored on a seperate ZFS filesystem than
	# the root filesysytem, we need to explicitly make this
	# directory available inside of the upgrade container,
	# which is what we're doing here.
	#
	# We need UPDATE_DIR to be available inside of the container so
	# that we can use the APT repository that it contains to upgrade
	# the packages inside the container, and verify that process.
	#
	cat >>"/etc/systemd/nspawn/$CONTAINER.nspawn" <<-EOF ||
		Bind=$UPDATE_DIR
	EOF
		die "failed to add '$UPDATE_DIR' to container config file"

	#
	# We also need to make "/domain0" available from within the
	# container, since that's required by the "upgrade-verify" JAR
	# that's run later. This allows the Virtualization upgrade
	# verification to verify the contents of it's database (e.g. run
	# flyway migration) is compatible with the new version of the
	# Virtualization application.
	#
	if [[ -d "/domain0" ]]; then
		cat >>"/etc/systemd/nspawn/$CONTAINER.nspawn" <<-EOF ||
			Bind=/domain0
		EOF
			die "failed to add '/domain0' to container config file"
	fi

	#
	# Lastly, we expose the host's DROPBOX_DIR to the container, so
	# that this directory can be used from within the container, to
	# store files that need to be preserved after the container is
	# destroyed; e.g. logs of the upgrade verification java process
	# are stored here (by default).
	#
	cat >>"/etc/systemd/nspawn/$CONTAINER.nspawn" <<-EOF ||
		Bind=$DROPBOX_DIR
	EOF
		die "failed to add '$DROPBOX_DIR' to container config file"

	#
	# We want to enable all available capabilities to the container
	# that we will use to run the ugprade verification. Ideally, we
	# would accomplish this by using "CAPABILITY=all" in the
	# "nspawn" configuration file generated above, but that's not
	# possible. The "CAPABILITY" field only understands actual
	# capabilities (e.g. CAP_SYS_ADMIN), and fails to parse the
	# special value "all".
	#
	# Thus, to workaround this limitation, we override the command
	# that is executed by systemd when starting the container's
	# service, such that it will pass the "--capability=all" option
	# to "systemd-nspawn".
	#
	mkdir -p "/etc/systemd/system/systemd-nspawn@$CONTAINER.service.d" ||
		die "failed to create container service override directory"
	cat >"/etc/systemd/system/systemd-nspawn@$CONTAINER.service.d/override.conf" <<-EOF ||
		[Service]
		ExecStart=
		ExecStart=/usr/bin/systemd-nspawn --quiet --boot --capability=all --machine=%i
	EOF
		die "failed to create container service override file"

	echo "$CONTAINER"
}

function get_mounted_rootfs_name() {
	local rootfs

	rootfs=$(zfs list -Ho name /)
	[[ -n "$rootfs" ]] ||
		die "failed to determine the mounted root filesystem name"

	echo "$rootfs"
}

function create_upgrade_container_in_place() {
	trap create_cleanup EXIT

	#
	# We want to use "/var/lib/machines" as the parent directory,
	# since that directory is used by "systemd-nspawn" when looking
	# for containers.  This way, we can create the directory and
	# mount our rootfs clone here, and "systemd-nspawn" will
	# automatically detect it.
	#
	DIRECTORY=$(mktemp -d -p "/var/lib/machines" -t delphix.XXXXXXX)
	[[ -d "$DIRECTORY" ]] || die "failed to create upgrade directory"
	CONTAINER=$(basename "$DIRECTORY")
	[[ -n "$CONTAINER" ]] || die "failed to obtain upgrade name"

	ROOTFS=$(get_mounted_rootfs_name)
	zfs snapshot "$ROOTFS@$CONTAINER" ||
		die "failed to create upgrade snapshot"
	zfs clone -o mountpoint="$DIRECTORY" \
		"$ROOTFS@$CONTAINER" \
		"rpool/ROOT/$CONTAINER" ||
		die "failed to create upgrade clone"

	create_upgrade_container_common
}

function create_upgrade_container_not_in_place() {
	trap create_cleanup EXIT

	#
	# We want to use "/var/lib/machines" as the parent directory,
	# since that directory is used by "systemd-nspawn" when looking
	# for containers.  This way, we can create the directory and
	# mount our new rootfs here, and "systemd-nspawn" will
	# automatically detect it.
	#
	DIRECTORY=$(mktemp -d -p "/var/lib/machines" -t delphix.XXXXXXX)
	[[ -d "$DIRECTORY" ]] || die "failed to create upgrade directory"
	CONTAINER=$(basename "$DIRECTORY")
	[[ -n "$CONTAINER" ]] || die "failed to obtain upgrade name"

	zfs create -o mountpoint="$DIRECTORY" \
		"rpool/ROOT/$CONTAINER" ||
		die "failed to create upgrade filesystem"

	#
	# This function needs to return the container's name to stdout,
	# so that consumers of this function/script can consume that
	# name and then later start/stop/destroy the container. Thus, we
	# have to redirect the output from debootstrap away from stdout.
	#
	# Also, we need to include the "systemd-container" package when
	# installing the base systemd with debootstrap so that starting
	# the container will work properly. Otherwise, after starting
	# the container, we won't be able to communicate and later run
	# commands in the container with "systemd-run".
	#
	debootstrap --no-check-gpg \
		--components=delphix --include=systemd-container \
		cosmic "$DIRECTORY" "file://$UPDATE_DIR/$IMAGE_VERSION/public" \
		1>&2 || die "failed to debootstrap upgrade filesystem"

	create_upgrade_container_common
}

function start() {
	if [[ ! -d "/var/lib/machines/$CONTAINER" ]] ||
		[[ ! -f "/etc/systemd/nspawn/$CONTAINER.nspawn" ]] ||
		[[ ! -d "/etc/systemd/system/systemd-nspawn@$CONTAINER.service.d" ]] ||
		! zfs list "rpool/ROOT/$CONTAINER" &>/dev/null; then
		die "container '$CONTAINER' non-existent or mis-configured"
	fi

	systemctl start "systemd-nspawn@$CONTAINER" ||
		die "failed to start container '$CONTAINER'"

	#
	# When starting the container above with "systemctl start", the
	# command will return prior to the container having fully
	# completed it's boot process. Thus, to ensure we don't attempt
	# the verification steps prior to the container being ready, we
	# first wait for the container to finish booting.
	#
	# shellcheck disable=SC2034
	for i in {1..600}; do
		if run /bin/systemctl is-active default.target &>/dev/null; then
			break
		fi

		sleep 1
	done

	run /bin/systemctl is-active default.target &>/dev/null ||
		die "'default.target' inactive in container '$CONTAINER'"
}

function stop() {
	systemctl stop "systemd-nspawn@$CONTAINER" ||
		die "failed to stop container: '$CONTAINER'"
}

function destroy() {
	rm -f "/etc/systemd/nspawn/$CONTAINER.nspawn" ||
		die "failed to remove file: '$CONTAINER.nspawn'"

	rm -rf "/etc/systemd/system/systemd-nspawn@$CONTAINER.service.d" ||
		die "failed to remove directory: $CONTAINER.service.d"

	if zfs list "rpool/ROOT/$CONTAINER" &>/dev/null; then
		zfs destroy "rpool/ROOT/$CONTAINER" ||
			die "failed to destroy container dataset: '$CONTAINER'"
	fi

	ROOTFS=$(get_mounted_rootfs_name)
	if zfs list "$ROOTFS@$CONTAINER" &>/dev/null; then
		zfs destroy "$ROOTFS@$CONTAINER" ||
			die "failed to destroy container snapshot: '$CONTAINER'"
	fi

	if [[ -d "/var/lib/machines/$CONTAINER" ]]; then
		rm -d "/var/lib/machines/$CONTAINER" ||
			die "failed to remove container directory: '$CONTAINER'"
	fi
}

function run() {
	systemd-run --machine="$CONTAINER" --quiet --wait --pipe -- "$@"
}

function get_bootloader_devices() {
	#
	# When installing/updating the bootloader during upgrade, we
	# need to determine which devices are being used as bootloader
	# devices. We determine this by listing the devices used by the
	# rpool. Additionally, we have to filter out devices that could
	# be attached to the rpool, but would never be used for the
	# bootloader. Finally, we need to strip off any parition
	# information, since we want to install the bootloader directly
	# to the device, rather than to a partition of the device.
	#
	zpool list -vH rpool |
		awk '! /rpool|mirror|replacing|spare/ {print $1}' |
		sed 's/[0-9]*$//'
}

function migrate_password_for_user() {
	local user="$1"
	local password

	password="$(awk -F: "\$1 == \"$user\" {print \$2}" /etc/shadow)"
	chroot "/var/lib/machines/$CONTAINER" usermod -p "$password" "$user" ||
		die "'usermod -p ... $user' failed for '$CONTAINER'"
}

function migrate_file() {
	local path="$1"
	local directory

	if [[ -f "$path" ]]; then
		directory="$(dirname "$path")"
		mkdir -p "/var/lib/machines/${CONTAINER}${directory}" ||
			die "'mkdir -p $directory' failed for '$CONTAINER'"
		cp "$path" "/var/lib/machines/${CONTAINER}${path}" ||
			die "'cp $path' failed for '$CONTAINER'"
	fi
}

function convert_to_bootfs_cleanup() {
	for dir in /proc /sys /dev; do
		umount -R "/var/lib/machines/${CONTAINER}${dir}" ||
			warn "'umount -R' of '$dir' failed"
	done
}

#
# The purpose of this function is to convert an existing upgrade
# container (specified by the $CONTAINER global variable) into the next
# boot filesystem used by the appliance; i.e. after calling this
# function, the container's root filesystem will be used as the
# appliance's root filesystem, the next time the appliance boots.
#
# This is done by updating the appliance's bootloader (i.e. grub) to
# point to the container's filesystem, along with setting the mountpoint
# of the filesystem to be "/" instead of "/var/lib/machines/$CONTAINER".
# This effectively removes the container, since systemd-nspawn only
# looks in /var/lib/machines, so it'll no longer find the container
# after the mountpoint changes.
#
function convert_to_bootfs() {
	trap convert_to_bootfs_cleanup EXIT

	for dir in /proc /sys /dev; do
		mount --rbind "$dir" "/var/lib/machines/${CONTAINER}${dir}" ||
			die "'mount --rbind' of '$dir' failed"
		mount --make-rslave "/var/lib/machines/${CONTAINER}${dir}" ||
			die "'mount --make-rslave' of '$dir' failed"
	done

	#
	# When performing a not-in-place upgrade, the root and delphix
	# users will not have any password by default. Thus, we need to
	# explicitly configure the passwords on the new root filesystem.
	# Here, we ensure the passwords for the root and delphix user on
	# the new root filesystem match their current values.
	#
	migrate_password_for_user delphix
	migrate_password_for_user root

	#
	# LX-72 Until we have a proper solution for migrating the
	# configuration of a given appliance to the new root filesystem
	# (i.e. any configuration that isn't supplied by a package), we
	# resort to explicitly migrating some of this configuration
	# here. The files listed here are dynamically generated and/or
	# modified by appliance-build.
	#
	while read -r file; do
		migrate_file "$file"
	done <<-EOF
		/etc/cloud/cloud.cfg.d/99-delphix-development.cfg
		/etc/cloud/cloud.cfg.d/99-delphix-internal.cfg
		/etc/issue
		/etc/nftables.conf
		/etc/ssh/sshd_config
		/etc/systemd/system/delphix-masking.service.d/override.conf
		/etc/systemd/system/delphix-mgmt.service.d/override.conf
		/etc/systemd/system/delphix-postgres@.service.d/override.conf
		/var/opt/delphix/server.conf
	EOF

	chroot "/var/lib/machines/$CONTAINER" update-grub ||
		die "'update-grub' failed in '$CONTAINER'"

	for dev in $(get_bootloader_devices); do
		[[ -e "/dev/$dev" ]] ||
			die "bootloader device '/dev/$dev' not found"

		[[ -b "/dev/$dev" ]] ||
			die "bootloader device '/dev/$dev' not block device"

		chroot "/var/lib/machines/$CONTAINER" \
			grub-install "/dev/$dev" ||
			die "'grub-install /dev/$dev' failed in '$CONTAINER'"
	done

	convert_to_bootfs_cleanup
	trap - EXIT

	zfs umount "rpool/ROOT/$CONTAINER" ||
		die "'zfs umount rpool/ROOT/$CONTAINER' failed"

	zfs set canmount=noauto "rpool/ROOT/$CONTAINER" ||
		die "'zfs set canmount=noauto rpool/ROOT/$CONTAINER' failed"

	zfs set mountpoint=/ "rpool/ROOT/$CONTAINER" ||
		die "'zfs set mountpoint=/ rpool/ROOT/$CONTAINER' failed"
}

function usage() {
	echo "$(basename "$0"): $*" >&2

	PREFIX_STRING="Usage: $(basename "$0")"
	PREFIX_NCHARS=$(echo -n "$PREFIX_STRING" | wc -c)
	PREFIX_SPACES=$(printf "%.s " $(seq "$PREFIX_NCHARS"))

	echo "$PREFIX_STRING create [in-place|not-in-place]"
	echo "$PREFIX_SPACES start <container>"
	echo "$PREFIX_SPACES stop <container>"
	echo "$PREFIX_SPACES destroy <container>"
	echo "$PREFIX_SPACES run <container> <command>"
	echo "$PREFIX_SPACES convert-to-bootfs <container>"

	exit 2
}

[[ "$EUID" -ne 0 ]] && die "must be run as root"

case "$1" in
create)
	[[ $# -lt 2 ]] && usage "too few arguments specified"

	case "$2" in
	in-place)
		[[ $# -gt 2 ]] && usage "too many arguments specified"
		create_upgrade_container_in_place
		;;
	not-in-place)
		[[ $# -lt 2 ]] && usage "too few arguments specified"
		[[ $# -gt 2 ]] && usage "too many arguments specified"
		create_upgrade_container_not_in_place
		;;
	*)
		usage "invalid create type specified: '$2'"
		;;
	esac

	;;
start)
	[[ $# -lt 2 ]] && usage "too few arguments specified"
	[[ $# -gt 2 ]] && usage "too many arguments specified"
	CONTAINER="$2"
	start
	;;
stop)
	[[ $# -lt 2 ]] && usage "too few arguments specified"
	[[ $# -gt 2 ]] && usage "too many arguments specified"
	CONTAINER="$2"
	stop
	;;
destroy)
	[[ $# -lt 2 ]] && usage "too few arguments specified"
	[[ $# -gt 2 ]] && usage "too many arguments specified"
	CONTAINER="$2"
	destroy
	;;
run)
	[[ $# -lt 3 ]] && usage "too few arguments specified"
	CONTAINER="$2"
	shift 2
	run "$@"
	;;
convert-to-bootfs)
	[[ $# -lt 2 ]] && usage "too few arguments specified"
	[[ $# -gt 2 ]] && usage "too many arguments specified"
	CONTAINER="$2"
	convert_to_bootfs
	;;
*)
	usage "invalid option specified: '$1'"
	;;
esac
