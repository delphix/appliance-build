#!/bin/bash
#
# Copyright 2018 Delphix
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

. "${BASH_SOURCE%/*}/common.sh"

IMAGE_PATH=$(get_image_path)
[[ -n "$IMAGE_PATH" ]] || die "failed to determine image path"

CONTAINER=

function create_cleanup() {
	#
	# Upon successful creation of the container, don't perform any
	# cleanup logic. Rather, the consumer of the script should
	# eventually "destroy" the container.
	#
	# shellcheck disable=SC2181
	[[ "$?" -eq 0 ]] && return

	#
	# If the CONTAINER variable is empty, this means the create
	# function failed before the container directory could be
	# created. Thus, there's nothing to clean up, so we can
	# immediately exit.
	#
	[[ -z "$CONTAINER" ]] && return

	destroy "$CONTAINER"
}

function get_mounted_rootfs_name() {
	local rootfs

	rootfs=$(zfs list -Ho name /)
	[[ -n "$rootfs" ]] ||
		die "failed to determine the mounted root filesystem name"

	echo "$rootfs"
}

function create_upgrade_container() {
	trap create_cleanup EXIT
	local type="$1"

	#
	# This will consume the "version.info" file from the unpacked
	# upgrade image, and will expose some global variables to this
	# script. We'll now have a "VERSION" global variable, which
	# contains the version of the "delphix-entire" package that is
	# contained in the upgrade image
	#
	source_version_information

	#
	# We want to use "/var/lib/machines" as the parent directory,
	# since that directory is used by "systemd-nspawn" when looking
	# for containers.  This way, we can create the directory and
	# mount our rootfs clone here, and "systemd-nspawn" will
	# automatically detect it.
	#
	DIRECTORY=$(mktemp -d -p "/var/lib/machines" -t "$VERSION.XXXXXXX")
	[[ -d "$DIRECTORY" ]] || die "failed to create upgrade directory"

	CONTAINER=$(basename "$DIRECTORY")
	[[ -n "$CONTAINER" ]] || die "failed to obtain upgrade name"

	ROOTFS_PARENT=$(dirname "$(get_mounted_rootfs_name)")
	zfs snapshot -r "$ROOTFS_PARENT@$CONTAINER" ||
		die "failed to create recursive upgrade container snapshot"

	zfs create \
		-o canmount=off \
		-o mountpoint=none \
		"rpool/ROOT/$CONTAINER" ||
		die "failed to create upgrade filesystem"

	case "$type" in
	in-place)
		zfs clone \
			-o canmount=noauto \
			-o compression=on \
			-o mountpoint="$DIRECTORY" \
			"$ROOTFS_PARENT/root@$CONTAINER" \
			"rpool/ROOT/$CONTAINER/root" ||
			die "failed to create upgrade / clone"

		zfs mount "rpool/ROOT/$CONTAINER/root" ||
			die "failed to mount upgrade / clone"
		;;
	not-in-place)
		zfs create \
			-o canmount=noauto \
			-o compression=on \
			-o mountpoint="$DIRECTORY" \
			"rpool/ROOT/$CONTAINER/root" ||
			die "failed to create upgrade / filesystem"

		zfs mount "rpool/ROOT/$CONTAINER/root" ||
			die "failed to mount upgrade / filesystem"

		#
		# This function needs to return the container's name to stdout,
		# so that consumers of this function/script can consume that
		# name and then later start/stop/destroy the container. Thus, we
		# have to redirect the output from debootstrap away from stdout.
		#
		# Also, we need to include the "systemd-container" package when
		# installing the base systemd with debootstrap so that starting
		# the container will work properly. Otherwise, after starting
		# the container, we won't be able to communicate and later run
		# commands in the container with "systemd-run".
		#
		debootstrap --no-check-gpg \
			--components=delphix --include=systemd-container \
			bionic "$DIRECTORY" "file://$IMAGE_PATH/public" 1>&2 ||
			die "failed to debootstrap upgrade filesystem"
		;;
	*)
		die "unsupported upgrade container type specified: '$1'"
		;;
	esac

	#
	# We require the contents of /export/home and /var/delphix to persist
	# across an upgrade, so we always use a clone of these datasets,
	# regardless of if we're creating an in-place or not-in-place upgrade
	# container.
	#
	zfs clone \
		-o canmount=on \
		-o compression=on \
		-o mountpoint="$DIRECTORY/export/home" \
		"$ROOTFS_PARENT/home@$CONTAINER" \
		"rpool/ROOT/$CONTAINER/home" ||
		die "failed to create upgrade /export/home clone"

	zfs clone \
		-o canmount=on \
		-o compression=on \
		-o mountpoint="$DIRECTORY/var/delphix" \
		"$ROOTFS_PARENT/data@$CONTAINER" \
		"rpool/ROOT/$CONTAINER/data" ||
		die "failed to create upgrade /var/delphix clone"

	mkdir -p "/etc/systemd/nspawn" ||
		die "failed to create directory: '/etc/systemd/nspawn'"

	#
	# We need to change some of the default configuration options
	# for the container, in order for it to behave as we expected:
	#
	# * PrivateUsers: We want the UIDs in the container to match the
	#   UIDs in the host. This way the container can execute commands
	#   (e.g. zfs) as the normal root user.
	#
	# * PrivateUsersChown: Since we set PrivateUsers to "no", we
	#   need to also ensure systemd-nspawn doesn't attempt to chown
	#   the containers root filesystem. Normally, with PrivateUsers
	#   set to "yes", the container's root filesystem would be
	#   chowned to match the UID mapping used by the container.
	#
	# * Private: We set networking to "private" so that the
	#   container can use ports without conflicting with ports that
	#   are already in used by the host.
	#
	# * Bind=/dev/zfs: We set this so that zfs/zpool/libzpool/etc.
	#   is usable from within the container.
	#
	cat >"/etc/systemd/nspawn/$CONTAINER.nspawn" <<-EOF ||
		[Exec]
		PrivateUsers=no
		[Network]
		Private=yes
		[Files]
		PrivateUsersChown=no
		Bind=/dev/zfs
	EOF
		die "failed to create container configuration file"

	#
	# Certain software (e.g. update-grub) running in the container
	# require access to the disk backing the root filesystem. Thus,
	# we expose this device to the container here; we're careful to
	# only expose it read-only, though, to prevent software in the
	# container from inadvertently corrupting the device.
	#
	for device in $(grub-probe --target=device /); do
		cat >>"/etc/systemd/nspawn/$CONTAINER.nspawn" <<-EOF ||
			BindReadOnly=$device
		EOF
			die "failed to add '$device' to container config file"
	done

	#
	# Since UPDATE_DIR is stored on a seperate ZFS filesystem than
	# the root filesysytem, we need to explicitly make this
	# directory available inside of the upgrade container,
	# which is what we're doing here.
	#
	# We need UPDATE_DIR to be available inside of the container so
	# that we can use the APT repository that it contains to upgrade
	# the packages inside the container, and verify that process.
	#
	cat >>"/etc/systemd/nspawn/$CONTAINER.nspawn" <<-EOF ||
		Bind=$UPDATE_DIR
	EOF
		die "failed to add '$UPDATE_DIR' to container config file"

	#
	# We also need to make "/domain0" available from within the
	# container, since that's required by the "upgrade-verify" JAR
	# that's run later. This allows the Virtualization upgrade
	# verification to verify the contents of it's database (e.g. run
	# flyway migration) is compatible with the new version of the
	# Virtualization application.
	#
	if [[ -d "/domain0" ]]; then
		cat >>"/etc/systemd/nspawn/$CONTAINER.nspawn" <<-EOF ||
			Bind=/domain0
		EOF
			die "failed to add '/domain0' to container config file"
	fi

	#
	# Lastly, we expose the host's DROPBOX_DIR to the container, so
	# that this directory can be used from within the container, to
	# store files that need to be preserved after the container is
	# destroyed; e.g. logs of the upgrade verification java process
	# are stored here (by default).
	#
	cat >>"/etc/systemd/nspawn/$CONTAINER.nspawn" <<-EOF ||
		Bind=$DROPBOX_DIR
	EOF
		die "failed to add '$DROPBOX_DIR' to container config file"

	#
	# We want to enable all available capabilities to the container
	# that we will use to run the ugprade verification. Ideally, we
	# would accomplish this by using "CAPABILITY=all" in the
	# "nspawn" configuration file generated above, but that's not
	# possible. The "CAPABILITY" field only understands actual
	# capabilities (e.g. CAP_SYS_ADMIN), and fails to parse the
	# special value "all".
	#
	# Thus, to workaround this limitation, we override the command
	# that is executed by systemd when starting the container's
	# service, such that it will pass the "--capability=all" option
	# to "systemd-nspawn".
	#
	mkdir -p "/etc/systemd/system/systemd-nspawn@$CONTAINER.service.d" ||
		die "failed to create container service override directory"
	cat >"/etc/systemd/system/systemd-nspawn@$CONTAINER.service.d/override.conf" <<-EOF ||
		[Service]
		ExecStart=
		ExecStart=/usr/bin/systemd-nspawn --quiet --boot --capability=all --machine=%i
	EOF
		die "failed to create container service override file"

	echo "$CONTAINER"
}

function start() {
	if [[ ! -d "/var/lib/machines/$CONTAINER" ]] ||
		[[ ! -f "/etc/systemd/nspawn/$CONTAINER.nspawn" ]] ||
		[[ ! -d "/etc/systemd/system/systemd-nspawn@$CONTAINER.service.d" ]] ||
		! zfs list "rpool/ROOT/$CONTAINER" &>/dev/null ||
		! zfs list "rpool/ROOT/$CONTAINER/root" &>/dev/null ||
		! zfs list "rpool/ROOT/$CONTAINER/home" &>/dev/null ||
		! zfs list "rpool/ROOT/$CONTAINER/data" &>/dev/null; then
		die "container '$CONTAINER' non-existent or mis-configured"
	fi

	systemctl start "systemd-nspawn@$CONTAINER" ||
		die "failed to start container '$CONTAINER'"

	#
	# When starting the container above with "systemctl start", the
	# command will return prior to the container having fully
	# completed it's boot process. Thus, to ensure we don't attempt
	# the verification steps prior to the container being ready, we
	# first wait for the container to finish booting.
	#
	# shellcheck disable=SC2034
	for i in {1..600}; do
		if run /bin/systemctl is-active default.target &>/dev/null; then
			break
		fi

		sleep 1
	done

	run /bin/systemctl is-active default.target &>/dev/null ||
		die "'default.target' inactive in container '$CONTAINER'"
}

function stop() {
	systemctl stop "systemd-nspawn@$CONTAINER" ||
		die "failed to stop container: '$CONTAINER'"
}

function destroy() {
	rm -f "/etc/systemd/nspawn/$CONTAINER.nspawn" ||
		die "failed to remove file: '$CONTAINER.nspawn'"

	rm -rf "/etc/systemd/system/systemd-nspawn@$CONTAINER.service.d" ||
		die "failed to remove directory: $CONTAINER.service.d"

	if zfs list "rpool/ROOT/$CONTAINER" &>/dev/null; then
		#
		# In order to safely perform the recursive destroy
		# below, we need to ensure the filesystems are unmounted
		# in the correct order. Since the "data" and "home"
		# datasets are mounted inside the "root" dataset, we
		# need to unmount these two datasets before attempting
		# to unmount (and/or destroy) the "root" dataset.
		#
		# Further, we don't check the return value of these
		# commands for simplicity's sake. If these fail, it
		# could be because they aren't mounted, or due to an
		# actual failure to unmount. If those datasets cannot be
		# unmounted, this will cause the destroy to fail below,
		# which we will catch, and then notify the user.
		#
		zfs unmount "rpool/ROOT/$CONTAINER/data" &>/dev/null
		zfs unmount "rpool/ROOT/$CONTAINER/home" &>/dev/null
		zfs unmount "rpool/ROOT/$CONTAINER/root" &>/dev/null

		zfs destroy -r "rpool/ROOT/$CONTAINER" ||
			die "failed to destroy container dataset: '$CONTAINER'"
	fi

	ROOTFS_PARENT=$(dirname "$(get_mounted_rootfs_name)")
	if zfs list "$ROOTFS_PARENT@$CONTAINER" &>/dev/null; then
		zfs destroy -r "$ROOTFS_PARENT@$CONTAINER" ||
			die "failed to destroy container snapshot: '$CONTAINER'"
	fi

	if [[ -d "/var/lib/machines/$CONTAINER" ]]; then
		rm -d "/var/lib/machines/$CONTAINER" ||
			die "failed to remove container directory: '$CONTAINER'"
	fi
}

function run() {
	systemd-run --machine="$CONTAINER" --setenv=CONTAINER="$CONTAINER" --quiet --wait --pipe -- "$@"
}

function get_bootloader_devices() {
	#
	# When installing/updating the bootloader during upgrade, we
	# need to determine which devices are being used as bootloader
	# devices. We determine this by listing the devices used by the
	# rpool. Additionally, we have to filter out devices that could
	# be attached to the rpool, but would never be used for the
	# bootloader. Finally, we need to strip off any parition
	# information, since we want to install the bootloader directly
	# to the device, rather than to a partition of the device.
	#
	zpool list -vH rpool |
		awk '! /rpool|mirror|replacing|spare/ {print $1}' |
		sed 's/[0-9]*$//'
}

function convert_to_bootfs_cleanup() {
	for dir in /proc /sys /dev; do
		umount -R "/var/lib/machines/${CONTAINER}${dir}" ||
			warn "'umount -R' of '$dir' failed"
	done
}

#
# The purpose of this function is to convert an existing upgrade
# container (specified by the $CONTAINER global variable) into the next
# boot filesystem used by the appliance; i.e. after calling this
# function, the container's root filesystem will be used as the
# appliance's root filesystem, the next time the appliance boots.
#
# This is done by updating the appliance's bootloader (i.e. grub) to
# point to the container's filesystem, along with setting the mountpoint
# of the filesystem to be "/" instead of "/var/lib/machines/$CONTAINER".
# This effectively removes the container, since systemd-nspawn only
# looks in /var/lib/machines, so it'll no longer find the container
# after the mountpoint changes.
#
function convert_to_bootfs() {
	trap convert_to_bootfs_cleanup EXIT

	mount --make-slave "/var/lib/machines/$CONTAINER" ||
		die "'mount --make-slave /var/lib/machines/$CONTAINER' failed"

	for dir in /proc /sys /dev; do
		mount --rbind "$dir" "/var/lib/machines/${CONTAINER}${dir}" ||
			die "'mount --rbind' of '$dir' failed"
		mount --make-rslave "/var/lib/machines/${CONTAINER}${dir}" ||
			die "'mount --make-rslave' of '$dir' failed"
	done

	chroot "/var/lib/machines/$CONTAINER" update-grub ||
		die "'update-grub' failed in '$CONTAINER'"

	for dev in $(get_bootloader_devices); do
		[[ -e "/dev/$dev" ]] ||
			die "bootloader device '/dev/$dev' not found"

		[[ -b "/dev/$dev" ]] ||
			die "bootloader device '/dev/$dev' not block device"

		chroot "/var/lib/machines/$CONTAINER" \
			grub-install "/dev/$dev" ||
			die "'grub-install /dev/$dev' failed in '$CONTAINER'"
	done

	convert_to_bootfs_cleanup
	trap - EXIT

	zfs umount "rpool/ROOT/$CONTAINER/data" ||
		die "'zfs umount rpool/ROOT/$CONTAINER/data' failed"
	zfs set mountpoint=/var/delphix "rpool/ROOT/$CONTAINER/data" ||
		die "'zfs set mountpoint rpool/ROOT/$CONTAINER/data' failed"

	zfs umount "rpool/ROOT/$CONTAINER/home" ||
		die "'zfs umount rpool/ROOT/$CONTAINER/home' failed"
	zfs set mountpoint=/export/home "rpool/ROOT/$CONTAINER/home" ||
		die "'zfs set mountpoint rpool/ROOT/$CONTAINER/home' failed"

	zfs umount "rpool/ROOT/$CONTAINER/root" ||
		die "'zfs umount rpool/ROOT/$CONTAINER/root' failed"
	zfs set mountpoint=/ "rpool/ROOT/$CONTAINER/root" ||
		die "zfs set mountpoint rpool/ROOT/$CONTAINER/root' failed"

	ROOTFS_PARENT=$(dirname "$(get_mounted_rootfs_name)")
	zfs set canmount=noauto "$ROOTFS_PARENT/data" ||
		die "'zfs set canmount $ROOTFS_PARENT/data' failed"
	zfs set canmount=noauto "$ROOTFS_PARENT/home" ||
		die "'zfs set canmount $ROOTFS_PARENT/home' failed"
	zfs set canmount=noauto "$ROOTFS_PARENT/root" ||
		die "'zfs set canmount $ROOTFS_PARENT/root' failed"
}

function migrate_password_for_user() {
	local user="$1"
	local password

	password="$(awk -F: "\$1 == \"$user\" {print \$2}" /etc/shadow)"
	chroot "/var/lib/machines/$CONTAINER" usermod -p "$password" "$user" ||
		die "'usermod -p ... $user' failed for '$CONTAINER'"
}

function migrate_file() {
	local path="$1"
	local parentdir

	if [[ -f "$path" ]]; then
		parentdir="$(dirname "$path")"
		mkdir -p "/var/lib/machines/${CONTAINER}${parentdir}" ||
			die "'mkdir -p $parentdir' failed for '$CONTAINER'"
		cp "$path" "/var/lib/machines/${CONTAINER}${path}" ||
			die "'cp $path' failed for '$CONTAINER'"
	fi
}

function migrate_configuration() {
	#
	# When performing a not-in-place upgrade, the root and delphix
	# users will not have any password by default. Thus, we need to
	# explicitly configure the passwords on the new root filesystem.
	# Here, we ensure the passwords for the root and delphix user on
	# the new root filesystem match their current values.
	#
	migrate_password_for_user delphix
	migrate_password_for_user root

	#
	# These files are generic OS files that are required for the
	# system to properly operate; these must be preserved acorss
	# upgrades.
	#
	# Note, the "/etc/resolv.conf" file is in this list, but the
	# "migrate_file" function will only migrate this path if it
	# points to a regular file. Often, this file will be a symlink,
	# in which case it won't be migrated, and we don't want it to
	# be. We only want to migrate this file if the admin (or some
	# software on the system) has converted it to a regular file,
	# and configured some host specific DNS settings, in which case
	# we want these settings to persist across the upgrade.
	#
	while read -r file; do
		migrate_file "$file"
	done <<-EOF
		/etc/hostid
		/etc/hostname
		/etc/hosts
		/etc/machine-id
		/etc/netplan/50-cloud-init.yaml
		/etc/resolv.conf
		/etc/ssh/ssh_host_dsa_key
		/etc/ssh/ssh_host_dsa_key.pub
		/etc/ssh/ssh_host_ecdsa_key
		/etc/ssh/ssh_host_ecdsa_key.pub
		/etc/ssh/ssh_host_ed25519_key
		/etc/ssh/ssh_host_ed25519_key.pub
		/etc/ssh/ssh_host_rsa_key
		/etc/ssh/ssh_host_rsa_key.pub
		/etc/sudoers.d/90-cloud-init-users
		/etc/zfs/zpool.cache
	EOF

	#
	# These files are specific to the delphix virtualization and/or
	# masking application, and must be preserved across upgrades.
	# Ideally this list would be provided by the applications
	# themselves, rather than specified here, but the infrastructure
	# to enable the applications to do this doesn't yet exist. Thus,
	# if the applications change in a way that requires changes to
	# this list, we can it up with a flag-day situation.
	#
	# Note, we migrate the "/etc/modprobe.d/20-zfs.conf" file without
	# verifying the tunables contained in that file make sense for the
	# new rootfs. We're reliant on this file being verified by some
	# other mechanism (e.g. the virtualization application's upgrade
	# verification), and failing the upgrade if the file's contents is
	# incompatible with the new appliance version. Without this
	# verification, the file could contain tunables that are not
	# appropriate for the new version, and could cause the appliance
	# to mis-behave after booting to the new rootfs.
	#
	while read -r file; do
		migrate_file "$file"
	done <<-EOF
		/etc/challenge_response.key
		/etc/cloud/cloud.cfg.d/99-disable-network-config.cfg
		/etc/delphix-interface-names.yaml
		/etc/default/locale
		/etc/engine-code
		/etc/engine-uuid
		/etc/engine.install
		/etc/fluent/fluent.conf
		/etc/issue
		/etc/krb5.conf
		/etc/krb5.keytab
		/etc/mds_upgrader
		/etc/modprobe.d/20-zfs.conf
		/etc/ntp.conf
		/etc/snmp/snmpd.conf
		/etc/rtslib-fb-target/saveconfig.json
		/var/opt/delphix/server.conf
	EOF

	#
	# The files listed here are dynamically generated and/or
	# modified by appliance-build.
	#
	while read -r file; do
		migrate_file "$file"
	done <<-EOF
		/etc/cloud/cloud.cfg.d/99-delphix-development.cfg
		/etc/cloud/cloud.cfg.d/99-delphix-internal.cfg
		/etc/nftables.conf
		/etc/systemd/system/delphix-masking.service.d/override.conf
		/etc/systemd/system/delphix-mgmt.service.d/override.conf
		/etc/systemd/system/delphix-postgres@.service.d/override.conf
	EOF
}

function usage() {
	echo "$(basename "$0"): $*" >&2

	PREFIX_STRING="Usage: $(basename "$0")"
	PREFIX_NCHARS=$(echo -n "$PREFIX_STRING" | wc -c)
	PREFIX_SPACES=$(printf "%.s " $(seq "$PREFIX_NCHARS"))

	echo "$PREFIX_STRING create [in-place|not-in-place]"
	echo "$PREFIX_SPACES start <container>"
	echo "$PREFIX_SPACES stop <container>"
	echo "$PREFIX_SPACES destroy <container>"
	echo "$PREFIX_SPACES run <container> <command>"
	echo "$PREFIX_SPACES convert-to-bootfs <container>"
	echo "$PREFIX_SPACES migrate-configuration <container>"

	exit 2
}

[[ "$EUID" -ne 0 ]] && die "must be run as root"

case "$1" in
create)
	[[ $# -lt 2 ]] && usage "too few arguments specified"
	[[ $# -gt 2 ]] && usage "too many arguments specified"

	case "$2" in
	in-place | not-in-place)
		create_upgrade_container "$2"
		;;
	*)
		usage "invalid create type specified: '$2'"
		;;
	esac

	;;
start)
	[[ $# -lt 2 ]] && usage "too few arguments specified"
	[[ $# -gt 2 ]] && usage "too many arguments specified"
	CONTAINER="$2"
	start
	;;
stop)
	[[ $# -lt 2 ]] && usage "too few arguments specified"
	[[ $# -gt 2 ]] && usage "too many arguments specified"
	CONTAINER="$2"
	stop
	;;
destroy)
	[[ $# -lt 2 ]] && usage "too few arguments specified"
	[[ $# -gt 2 ]] && usage "too many arguments specified"
	CONTAINER="$2"
	destroy
	;;
run)
	[[ $# -lt 3 ]] && usage "too few arguments specified"
	CONTAINER="$2"
	shift 2
	run "$@"
	;;
convert-to-bootfs)
	[[ $# -lt 2 ]] && usage "too few arguments specified"
	[[ $# -gt 2 ]] && usage "too many arguments specified"
	CONTAINER="$2"
	convert_to_bootfs
	;;
migrate-configuration)
	[[ $# -lt 2 ]] && usage "too few arguments specified"
	[[ $# -gt 2 ]] && usage "too many arguments specified"
	CONTAINER="$2"
	migrate_configuration
	;;
*)
	usage "invalid option specified: '$1'"
	;;
esac
